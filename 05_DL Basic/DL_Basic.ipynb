{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 목차\n",
    "1. DL Basic Terms\n",
    "2. Historical DL trand review\n",
    "3. Neural Network structure (MLP)\n",
    "4. Optimization (최적화)\n",
    "5. Gradient Descent Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 : DL Basic Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -01 : Introduction\n",
    "- Implementation skills (tools)\n",
    "    - tensorflow\n",
    "    - pytorch\n",
    "- Math skills\n",
    "    - Linear Algrebra (선형대수학)\n",
    "    - Probability (확률론)\n",
    "- Knowing a lot of recent PAPERS (최신논문경향)\n",
    "    - 14 ~ 15 기본 논문내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -02 : Contents\n",
    "- 방향성\n",
    "    - 각 주제의 간략한 설명\n",
    "    - 해당 주제의 논문들을 찾아가는 방법\n",
    "- 주제들\n",
    "    1. Historycal Review\n",
    "    2. Neural Network & Multi-Layer Perceptron\n",
    "    3. Optimization Methods\n",
    "    4. Convolutional Neural Networks\n",
    "    5. Modern CNN\n",
    "    6. Compoter Vsion Applications\n",
    "    7. Recurrent Neural Network\n",
    "    8. Transfomer\n",
    "    9. Generative Models Part-1\n",
    "    10. Generative Models Part-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -03 : 인공지능 분류\n",
    "- Altificial Inteligence (AI) (인공지능) : 인간의 지능을 모방하는 것\n",
    "    - Machine Learing (ML) (기계학습) : 데이터 중심 접근법 \n",
    "        - Deep Learing (DL) (딥러닝) : 인공 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -04 : Deep Learing Key Conmponents\n",
    "- 새로운 논문과 연구를 바라볼떄 이 네가지를 기준으로 바라보자\n",
    "    - The *Data* that the model can learn from : \n",
    "    - The *Model* how to transfrom the data :\n",
    "    - The *Loss* function that quantifies the badness of the model :\n",
    "    - The *Algorithm* to adjust the parameters to minimize the loss :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _01 : DATA\n",
    "- Data depend on the type of the problem to slove\n",
    "    - Classification\n",
    "    - Semantic Segmentation : 각 이미지의 픽셀별로 분류하는것\n",
    "    - Detection : 이미안의 물체의 바운딩 박스를 찾는것\n",
    "    - Pose Estimation : 이지안의 사람의 2,3차원 스켈레톤 정도를 찾는것\n",
    "    - Visual QnA : 질문에 대한 답을 찾는것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _02 : Model\n",
    "- 이미지, 단어, 텍스트 등을 원하는 형태의 Label로 바꿔주는 도구\n",
    "    - 모델의 성징에 따라서 결과가 달라지고 그 안에 다양한 테크닉이 있음\n",
    "        - EX\n",
    "            - Alex, GoogLeNet, ResNet, DenseNet, LSTM, Deep AutoEncoders, GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _03 : Loss function\n",
    "- 이루고자 하는것의 근사치\n",
    "- 모델과 데이터가 절해져있을때 이 모델을 어떻게 학습할지 여부\n",
    "- 해당 loss Function 을 통상적이라는 이유로 무작정 사용하는것이 아닌 사용되는 이유와 필요시 다른 loss Function 을 적용할 수 있어야 한다\n",
    "\n",
    "    - Loss Function EX\n",
    "        - Regression Task\n",
    "            - MSE (Mean Square Error) = $\\displaystyle{\\frac{1}{N}\\sum_{i=1}^{n}\\sum_{d=1}^{D}({y_i}^{(d)}-{\\hat{y}_i}^{(d)})^2}$\n",
    "        - Classification Task\n",
    "            - CE (Cross Entropy) = $\\displaystyle{-\\frac{1}{N}\\sum_{i=1}^{n}\\sum_{d=1}^{D}{y_i}^{(d)}\\log{\\hat{y}_i}^{(d)}}$\n",
    "        - Probabilistic Task\n",
    "            - MLE (Maximum Likelihood Estimation) = $\\displaystyle{\\frac{1}{N}\\sum_{i=1}^{n}\\sum_{d=1}^{D}\\log{\\Nu{}({y_i}^{(d)}\\\\;{\\hat{y}_i}^{(d)},1)} \\quad}$ (=MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _04 : optimization Algorithm (최적화 방법)\n",
    "- 데이터, 모델, loss func 가 주어져있을때 네트워크를 어떻게 줄일지 고민하는 것\n",
    "- 또한 한번도 보지못한 테스트 데이터에서도 잘 동작할 수 있도록 하는 것\n",
    "\n",
    "    - EX (오버피팅 방지)\n",
    "        - Dropout\n",
    "        - Early stopping\n",
    "        - K-fold validation\n",
    "        - Weight decay\n",
    "        - Batch normalization\n",
    "        - Mixup\n",
    "        - Ensemble\n",
    "        - Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 : Historical DL trand review\n",
    "- 2012 - AlexNet\n",
    "- 2013 - DQN\n",
    "- 2014 - Encoder / Decoder / Adam\n",
    "- 2015 - GAN, ResNet(Residual Network)\n",
    "- 2016 - \n",
    "- 2017 - Transformer\n",
    "- 2018 - Bert\n",
    "- 2019 - Big Language Model (GPT-X)\n",
    "- 2020 - Self Supervised Learning (SimCLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 : Neral Network structure (MLP)\n",
    "- Neural network are computing system veguely inspired by the biological neural networks that constitute animal brains\n",
    "    - 신경망은 동물 뇌를 구성하는 생물학적 신경 네트워크에서 영감을 얻은 컴퓨팅 시스템입니다.\n",
    "- Neural networks are function approximators that stack affine transformations followed by nonlinear transformations\n",
    "    - 신경망은 아핀 변환과 비선형 변환을 쌓는 기능 근사기입니다.\n",
    "    - 뉴럴 네트워크는 함수를 모방하는 function approximator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -01 : Linear Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's start with the most simple example\n",
    "\n",
    "    - Data : $\\displaystyle{D \\; = \\; \\{(x_i\\,y_i)\\}_{i=1}^N}$\n",
    "    - Model : $\\displaystyle{\\hat{y} \\; = \\; \\vec{w}\\cdot\\vec{x} \\; + \\; \\vec{b}}$\n",
    "    - Loss : $\\displaystyle{\\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^N(y_i \\; - \\; \\hat{y}_i)^2}$\n",
    "\n",
    "        - so, find $\\vec{w} \\; \\& \\; \\vec{b}$\n",
    "\n",
    "        - Partial deivative \n",
    "             > $\\displaystyle{\\frac{\\partial{\\mathcal{L}}}{\\partial{w}} = \\frac{\\partial{}}{\\partial{w}}{\\frac{1}{N}}\\sum_{i=1}^N(y_i \\; - \\; \\hat{y}_i)^2}\\\\$  \n",
    "             > $\\displaystyle{\\qquad = \\frac{\\partial{}}{\\partial{w}}\\frac{1}{N}\\sum_{i=1}^N(y_i \\; - \\; wx_i \\; - \\; b)^2}\\\\$  \n",
    "             > $\\displaystyle{\\qquad = \\frac{\\partial{}}{\\partial{w}}\\frac{1}{N}\\sum_{i=1}^N(y_i^2 \\; -2x_iy_iw \\; -2by_i \\; + x_i^2w^2 \\; + 2bx_iw \\; + b^2)}\\\\$  \n",
    "             > $\\displaystyle{\\qquad = \\frac{\\partial{}}{\\cancel{\\partial{w}}}\\frac{1}{N}\\sum_{i=1}^N(\\cancel{y_i^2} \\; -2x_iy_i\\cancel{w} \\; \\cancel{-2by_i} \\; + 2 \\times x_i^2\\cancel{w^2} \\times w \\; + 2bx_i\\cancel{w} \\; + \\cancel{b^2})}\\\\$  \n",
    "             > $\\displaystyle{\\qquad = \\frac{1}{N}\\sum_{i=1}^N(-2y_i \\; + 2x_iw \\; + 2b)x_i}\\\\$  \n",
    "             > $\\displaystyle{\\qquad = \\frac{1}{N}\\sum_{i=1}^N-2(y_i \\; - wx_i \\; - b)x_i}$  \n",
    "\n",
    "             > $\\displaystyle{\\frac{\\partial{\\mathcal{L}}}{\\partial{b}} = \\frac{\\partial{}}{\\partial{b}}\\sum_{i=1}^N(y_i \\; - \\; \\hat{y}_i)^2}\\\\$  \n",
    "             > $\\displaystyle{\\qquad = \\frac{\\partial{}}{\\partial{b}}\\frac{1}{N}\\sum_{i=1}^N(y_i \\; - \\; wx_i \\; - \\; b)^2}\\\\$  \n",
    "             > $\\displaystyle{\\qquad = \\frac{1}{N}\\sum_{i=1}^N-2(y_i \\; - wx_i \\; - b)}$  \n",
    "\n",
    "             > $\\displaystyle{w \\leftarrow w - \\eta\\frac{\\partial{\\mathcal{L}}}{\\partial{w}}} \\\\$  \n",
    "             > $\\displaystyle{b \\leftarrow b - \\eta\\frac{\\partial{\\mathcal{L}}}{\\partial{b}}} \\\\$  \n",
    "             >> - $\\displaystyle{\\leftarrow}$ : Update  \n",
    "             >> -  $\\\\\\displaystyle{\\eta}$ : Stepsize (learning rate)  \n",
    "\n",
    "- Of course, we can handel multi dimensional input and out put\n",
    "\n",
    "    > $\\displaystyle{\\mathbb{Y}_{m \\times n} \\; = \\; \\mathbb{X}_{m \\times i} \\cdot \\mathbb{W}^T_{i \\times n} + \\mathbb{B}_{m \\times n}} \\\\$  \n",
    "    > $\\displaystyle{\\qquad\\qquad = \\mathbb{X}_{m \\times i}\\overset{\\{\\mathbb{W}_{i \\times n}\\\\,\\mathbb{b}\\}}{\\to}\\mathbb{Y}_{m \\times n}}$  \n",
    "\n",
    "    - But, What if we stack more ?\n",
    "        - We need nonlinearity (활성함수를 통한 변환)\n",
    "            - ReLU\n",
    "                > 양수 = 그대로, 음수 = 0\n",
    "            - Sigmoid\n",
    "                > $\\displaystyle{\\frac{1}{1 \\; + \\; e^{-x}}}$\n",
    "            - Hyperbolic Tangent\n",
    "                > $\\displaystyle{\\frac{e^x \\; - \\; e^{-x}}{e^x \\; + \\; e^{-x}}}$\n",
    "\n",
    "    - Deep Learning 이 강력한 이유?\n",
    "        - There is a single hidden layer feedforward network that approximates any measureable function to any desired dgree of accuracy on some compact set K.\n",
    "        - 히든 레이어가 하나만 있는 뉴럴 네트워크라 하더라도 모든 선형식을 표현할 수 있다\n",
    "            - 하지만 그 표현식이 우리가 원하는 식이라고는 보장되지 않는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 : Optimization (최적화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -01 : Gradient Descent (경사하강법)\n",
    "- First-order iterative optimization alorithm for finding a local minimum of a differentiable function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -02 : Important Concept in *Optimization*\n",
    "- Generalization\n",
    "- Under-fitting vs Over-fitting\n",
    "- Cross Validation\n",
    "- Bias-variance trade-off\n",
    "- Bosstrapping\n",
    "- Bagging and boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _01 : Generalization (일반화)\n",
    "\n",
    "- Generalization Gap\n",
    "    - Train set Error 와 Test set Error 의 갭\n",
    "    - Generalization gap 이 적다 : 학습데이터와 비슷한 수준의 실제 데이터화 결과를 보여줄 것이다 예측 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _02 : Under vs Over fitting\n",
    "- Over fitting\n",
    "    - Train 에서 잘 동작 , Test 에서 잘 동작하지 않음\n",
    "- Under fitting\n",
    "    - 학습이 부족한 상황"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _03 : Cross-validation\n",
    "- Cross validation is a model validation technique for assessing how the model will generalize to an idependent (test) data set\n",
    "\n",
    "- EX module\n",
    "    > from sklearn.model_selection import train_test_split  \n",
    "    > from sklearn.model_selection import KFold  \n",
    "    > from sklearn.model_selection import StratifiedKFold  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _04 : Bias & Variance (trade off)\n",
    "\n",
    "- Bias\n",
    "    - 평균적으로 보았을때 True 에 근접한 정도  \n",
    "\n",
    "- Variace\n",
    "    - 복작성\n",
    "    - 변화정도  \n",
    "\n",
    "- Bias & Variance Trade-off\n",
    "    - Given $\\mathbb{D} = \\{(x_i, \\; t_i)\\}_{i=1}^{N}$, where $t \\; = \\; f(x) + \\epsilon{} \\;$ and $\\; \\epsilon{} \\;$ ~ $\\; \\mathbb{N}(0, \\; \\sigma{}^2) $  \n",
    "    - We can derive that what we are minimizing ( $cost$ ) can be decomposed into three different parts : $bias^2, \\; variance,$ and $noise$\n",
    "        - 학습 데이터에 노이즈가 포함되어 있을때 노이즈가 껴있는 타겟데이터를 미니마이즈하는것은 3가지 가 연관되어 있으며 상호보완적인 관계로 하나를 줄이면 하나는 늘어날 수 밖에 없다\n",
    "            - ex. Precision vs Recall\n",
    "        > $\\displaystyle{\\underbrace{\\mathbb{E}[(t-\\hat{f}^2)]}_{Cost} \\; = \\; E[(t \\; - \\; f \\; + \\; f \\; - \\; \\hat{f})^2]} \\\\$  \n",
    "        > $\\displaystyle{\\qquad\\qquad = \\quad \\cdots} \\\\$  \n",
    "        > $\\displaystyle{\\qquad\\qquad = \\quad \\underbrace{\\mathbb{E}[(f \\; - \\; \\mathbb{E}[\\hat{f}]^2)^2]}_{bias^2} \\; + \\; \\underbrace{\\mathbb{E}[(\\mathbb{E}[\\hat{f}]^2 \\; - \\; \\hat{f})^2]}_{variance} \\; + \\; \\underbrace{\\mathbb{E}[\\epsilon{}]}_{noise}} \\\\$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _05 : Bootstrapping\n",
    "- Bootstrapping is any test or metric that uses random sampling with replacement\n",
    "- 고정되어있는 학습데이터에서 서브 샘플링을 통해서 학습데이터를 여러개를 생성하여 여러 모델을 만들어 학습을 진행하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _06 : Bagging vs Boosting\n",
    "- Bagging (*B*ootstrapping *agg*regat*ing*)\n",
    "    - Multiple models are being trained with bootstrapping\n",
    "    - ex) Base classifier are fitted on random subset where indiviual predictions are aggregated (voting or averaging)\n",
    "    - 앙상블 (Ensemble)\n",
    "- Boosting\n",
    "    - It focuses on those specific training samples that are hard to classify.\n",
    "    - A strong model is built bycombining weak learner in sequence where each learner from the mistakes of the previous weak learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05  : Gradient Descent Method\n",
    "\n",
    "- Stochastic gradient descent\n",
    "    - Update with the gradient computed from a single sample\n",
    "\n",
    "- Mini-batch gradient descent\n",
    "    - UPdate with the gradient computed form a subset of data\n",
    "\n",
    "- Batch gradient descent\n",
    "    - Update with the gradient computed from whole data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -01 : Batch-size Matters\n",
    "\n",
    "- It has been observes in practice that when using a larger bstch there is a degradation in the quality of the model, as measured by its ability to generalized\n",
    "\n",
    "- We ... present numerical evidence that suepports the view that large batch methods tend to converge to *sharp minimizers* of the training and testing functions. In contrast, small-batch methods consistently converge to *flat minimizers* ... this is due to the inherent noise in the gradient estimation.\n",
    "    - 경사하강법의 특성(노이즈) 에 의해 Flat Mininum 는 train 과 test 의 결과가 비슷하게 나오는경향이 있다\n",
    "    - Sharp Minimum 은 차이가 크다\n",
    "        - Generalize performance 차이가 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -02 : Gradient Descent Optimizer (경사하강법 최적화)\n",
    "\n",
    "- (Stochastic) Gradient descent\n",
    "- Momentum\n",
    "- Nesterov accelerated Gradient\n",
    "- Adagrad\n",
    "- Adadelta\n",
    "- RMSprop\n",
    "- Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _01 : (Stochastic) Gradient Descent\n",
    "\n",
    "- 기존의 형태\n",
    "    > $W_{t+1} \\; \\leftarrow \\; W_t \\; - \\eta{}g_t$  \n",
    "    > $\\qquad\\qquad\\qquad\\quad \\downarrow \\;\\; \\searrow$  \n",
    "    > $\\quad\\; Learning rate \\quad Gradient$  \n",
    "    - 문제점 : Learning rate에 따른 학습의 영향이 너무 컷다\n",
    "        - 해결방안 : Optimization technique 개발 (ex. Momentum : 학습 관성적용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _02 : Momentom\n",
    "\n",
    "- 기존의 learning rate 에 의존한 Gradient descent 학습법의 문제점을 개선하기 위해 고안됨\n",
    "- 관성을 이용\n",
    "    - 기울기에 가속도를 부여 : Learning rate 를 점진적을 증가 (관성)\n",
    "        > $\\underline{a_{t+1}} \\; \\leftarrow \\; \\underline{\\beta{}}a_t \\; + \\; \\underline{g_t} \\; \\cdots \\; Gradient$  \n",
    "        > $\\vdots \\qquad\\qquad \\ddots$  \n",
    "        > $Accumulation \\;\\;  momentum $  \n",
    "        >> $W_{t+1} \\; \\leftarrow \\; W_t \\; - \\eta{}\\underline{a_t}$  \n",
    "        >> 진동을 하는 경향이 있지만 잘 학습하는 경향을 보임\n",
    "        - 문제점 : 지난 학습결과에 대한 Momentum 으로 관성이 과대해져 최저점을 지나치고 진동하는 경우가 자주 발생\n",
    "            - 해결방안 : Momentum 적용시점 변경 (ex. Nestrov Accelerated Gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _03 : Nesterov Accelerated Gradient\n",
    "\n",
    "- Momentum 의 Learing rate 의 가속기능때문에 진동이 증가하는 현상을 감쇄하기 위해 고안됨\n",
    "    - Momentum : t+1 의 momentum 을 t 번째에서 선정\n",
    "    - Nesterov accelerated Gradient : t+1 의 mometum dmf t+1 번째의 위치에서 선정\n",
    "        > $a_{t+1} \\; \\leftarrow \\beta{}a_t \\; + \\; \\underbrace{\\nabla{}\\mathcal{L}(W_t \\; - \\; \\eta{}\\beta{}a_t)}_{Lookahead gradient}$  \n",
    "        >> $W_{t+1} \\; \\leftarrow W_t \\; - \\; \\eta{}a_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _04 : Adagrad\n",
    "\n",
    "- *Adagrad* adapts the learning rate, performing larger ipdates for inrequent and smaller updates for frequent parameters.  \n",
    "    - 각각의 파라미터의 변화정도를 저장하여 많이 변화한것은 적게 변환시키고 , 적게 변화한것은 많이 변화시켜가는것\n",
    "        > $\\displaystyle{W_{t+1} \\; = \\; W_t \\; - \\; \\frac{\\eta{}}{\\sqrt{\\underbrace{G_t}_{Sum \\; of \\; gradient \\; squares} \\; + \\; \\underbrace{\\epsilon{}}_{for \\; numerical \\; stability}}}g_t}$  \n",
    "        - 문제점 : 학습을 진행하면 할 수록 $\\underbrace{G_t}_{Sum \\; of \\; gradient \\; squares}$ 가 증가하여 학습이 중단되는 현상이 발생하게 된다\n",
    "            - 해결방안 : $G_t$ 의 과대 증가 방지 (ex. Adadelta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _05 : Adadelta\n",
    "\n",
    "- *Adadelta* extends *Adagrad* to reduce its monotonically decreasing the learning rate restrcing the accumulation window\n",
    "    - Adagrad 의 계속해서 증가하는 $G_t$ 를 방지\n",
    "    - 문제점 : 묶음(Sum of gradient squares)로써 가 아니라면 window size 만큼의 파라미터 정보를 갖고있어야 하므로 이것만으로도 데이터가 지나치게 방대해져 버린다\n",
    "        -  $\\gamma{}$ 를 사용하여 대략적인 평균값을 저장한다\n",
    "    - 특징 : Learning rate 가 없음\n",
    "        - 튜닝이 가능한 요소가 많지 않기 때문에 자주 사용되지 않음\n",
    "            > $ \\displaystyle{\\quad \\underbrace{G_t}_{EMA \\; of \\; gradient \\; squares} \\; = \\; \\gamma{}G_{t-1} \\; + \\; (1 \\; - \\; \\gamma{})g^2}$ : 대략적인 평균값?  \n",
    "            >> $ \\displaystyle{\\qquad\\qquad W_{t+1} \\;\\qquad\\qquad = \\; W_t \\; - \\; \\frac{\\sqrt{H_{t-1} \\; + \\; \\epsilon{}}}{\\sqrt{\\underline{G_t} \\; + \\; \\epsilon{}}}g_t}$  \n",
    "            >> $ \\displaystyle{\\quad \\underbrace{H_t}_{EMA \\; of \\; difference \\; squares} \\; = \\; \\gamma{}H_{t-1} \\; + \\; (1 \\; - \\; \\gamma{})(\\Delta{}W_t)^2}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _06 : RMSprop\n",
    "\n",
    "- *RMSprop* is an unpublished, adaptive learning rate method propsed by Geoff Hinton in his lecture\n",
    "    - 별도의 논문은 없고 실험적으로 집러닝 렉쳐에서 좋은 결과를 나타내어 유행이 되었던 학습방법\n",
    "        > $ \\displaystyle{\\underbrace{G_t}_{EMA \\; of \\; gradeint \\; squares} \\; = \\; \\gamma{}G_{t-1} \\; + \\; (1 \\; - \\; \\gamma{})g^2t}$  \n",
    "        >> $ \\displaystyle{\\qquad\\quad W_{t+1} \\quad\\quad\\; = \\; W_t \\; - \\; \\frac{\\overbrace{\\eta{}}^{step \\; size}}{\\sqrt{\\underline{G_t} \\; + \\; \\epsilon{}}}}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _07 : Adam\n",
    "\n",
    "- Adaptive Moment Estimaation (Adam) leverages both past gradients and squared gradients\n",
    "    - Adam effectively combines momentum with adaptive learning rate approach\n",
    "        > $ \\displaystyle{\\qquad\\;\\underbrace{m_t}_{momentum} \\qquad\\;\\; = \\; \\beta{}_1m_{t=1} \\; + \\; (1 \\; - \\; \\beta{}_1)g_t}$  \n",
    "        > $ \\displaystyle{\\underbrace{v_t}_{EMA \\; of \\; gradient \\; squares} \\; = \\; \\beta{}_2v_{t-1} \\; + \\; (1 \\; - \\; \\beta{}_2)g^2_t}$  \n",
    "        >> $ \\displaystyle{W_{t+1} \\; = \\; W_t \\; - \\; \\frac{\\overbrace{\\eta{}}^{step \\; size}}{\\sqrt{\\underline{v_t} \\; + \\epsilon{}}}\\frac{\\sqrt{1 \\; - \\; \\beta{}^t_2}}{1 \\; - \\; \\beta{}^t_1}\\underline{m_t}}$  \n",
    "        >>> $ ※ \\; default \\; \\epsilon{} \\:: 10^{-7}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 : Regularization\n",
    "\n",
    "- 규제화\n",
    "    - 학습에 방해되도록 규제를 거는 것\n",
    "\n",
    "- 목적\n",
    "    - Generalization 이 잘 되도록 하는것\n",
    "    - Train Function 의 결과가 Test set 에서도 잘 적용되도록 하는것\n",
    "\n",
    "- 종류\n",
    "    - Early Stopping\n",
    "    - Parameter norm penalty\n",
    "    - Data augmentation\n",
    "    - Noise robustness\n",
    "    - Label smoothing\n",
    "    - Dropout\n",
    "    - Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -01 : Early Stopping\n",
    "\n",
    "- 적용 목적\n",
    "    - Generalization 이 잘 되도록 조기에 학습을 종료하는 기능\n",
    "\n",
    "        <img src='../image/earlystopping_02.jpeg' width=400>\n",
    "\n",
    "    - 과도한 학습은 Train set에 대한 성능에 좋은영향을 주지만 일정수준이상의 학습은 Test(실제)와 간극이 발생하게 된다 (Over fitting) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -02 : Parameter Norm Penalty\n",
    "\n",
    "- It adds smoothness to the function space\n",
    "\n",
    "    - 의미1 : 뉴럴 네트워크의 함수를 최대한 부드러운 함수로 만들자\n",
    "\n",
    "    - 의미2 : 부드러운 함수일 수록 Generalization performance 가 높을 것이다\n",
    "\n",
    "        - 뉴럴 네트워크 파라미터가 너무 커지지않도록 하는것 (대신 깊어지도록)\n",
    "\n",
    "            > $\\displaystyle{Total \\; cost \\; = \\; Loss \\; (\\; D\\; \\;; \\; W \\; ) \\; + \\underbrace{\\frac{\\alpha{}}{2}||W^2||_2}_{Parameter \\; Norm \\; Penalty}}$\n",
    "\n",
    "            - 식 뜻 : 네트워크 파라미터를 다 제곱 후 더한 값이 작으면 작을 수록 좋다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -03 : Data Augmentation\n",
    "\n",
    "- More data are always welcomedgfh\n",
    "    - 기존의 머신러닝에서는 많은 데이터가 있더라도 모든 데이터에 대해 표현의 방법에 한계가 있었으나, 딥러닝에서는 그 한계가 상대적으로 많이 줄고 표현력이 늘었기에 많은 양의 데이터일 수록 더욱 효과적이다\n",
    "    - 반대로 표현하면 적은양의 데이터일 경우 기존의 머신러닝 알고리즘을 활용하는것이 더 효과적일 수 있다\n",
    "\n",
    "        <img src='../image/data_agmentation_01.jpeg' width=700>\n",
    "        \n",
    "\n",
    "\n",
    "- However, in most cases, training data are given in advance\n",
    "    - 하지만, 많은 경우에서 train 데이터의 양은 한정적으로 제공된다\n",
    "    \n",
    "\n",
    "- In such cases, we need data augmentation\n",
    "    - 이러한 경우 우리는 *Data Agumentation* 이 필요하다\n",
    "        - 예시 -1 )\n",
    "            - 기존의 데이터를 지지고 볶아서 새로운 데이터(label 이 변환되지 않는 한도 내에서)를 생성하여 부족한 데이터를 보충한다\n",
    "            \n",
    "                <img src='../image/data_agmentation_02.jpeg' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -04 : Noise Robustness\n",
    "\n",
    "- 수행 방법\n",
    "    - Add random noises inputs or weights\n",
    "        - 데이터와 뉴럴네트워크 $w$ 에도 노이즈 값을 삽이하여 학습을 진행\n",
    "            - 뉴럴 네트워크를 학습시킬때 노이즈 값을 통해 학습을 흔들어 주면 실험적으로 성능향상이 되는 경향을 보여왔었다\n",
    "                - 왜 잘되는지에 대해서는 의문모름\n",
    "\n",
    "                    <img src='../image/noise_robustness_01.jpeg' width=400>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -05 : Label Smoothing\n",
    "\n",
    "- Mix-up constructs augmented training examples by mixing both input and output of two randomly selcted training data\n",
    "    - train 단계의 학습데이터를 뽑아서 섞어주는 것\n",
    "    - 분류의 경계를 모호하게 하는것\n",
    "\n",
    "        <img src='../image/label_smothing_01.jpeg' height=200>  <img src='../image/label_smothing_02.jpeg' height=200>\n",
    "\n",
    "- 목적\n",
    "    - 분류문제에서 데이터가 한정적이고 더이상 데이터를 얻기 어려울때 활용하기 좋음\n",
    "        - *어떻게 활용되고 어떤점에서 좋은건지 솔직히 잘 이해가 안됨*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -06 : Dropout\n",
    "\n",
    "- 뉴럴네트워크의 특정 노드의 웨이트를 0으로 변경하는것\n",
    "    - In each forward pass, randomly set some neurons to zero\n",
    "\n",
    "        <img src='../image/dropout_01.jpeg' height=200>\n",
    "\n",
    "- 목적\n",
    "    - 각각의 뉴런들이 robustenss(견고해) 진다고 생각할 수 있다?\n",
    "        - 뭔말이야 이게 설명 개같네\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -07 : Batch Normalization\n",
    "\n",
    "- Batch normalization compute the empirical mean and variance independently for each dimension (layer) and normalize\n",
    "    - 배치 정규화 각 차원 (레이어)에 대해 독립적으로 경험적 평균과 분산을 계산하고 정규화\n",
    "    - 적용하고자하는 layer의 statistic(통계) 을 정규화\n",
    "        - 진짜 뭔말이야 \n",
    "\n",
    "        > $\\displaystyle{\\mu{}_B \\; = \\; \\frac{1}{m}\\sum_{i=1}^{m}}x_i$  \n",
    "        > $\\displaystyle{\\sigma{}^2_B \\; = \\; \\frac{1}{m}\\sum_{i=1}^{m}(x_i \\; - \\; \\mu{}_B)^2}$  \n",
    "        > $\\displaystyle{\\hat{x}_i \\; = \\; \\frac{x_i \\; - \\; \\mu{}_B}{\\sqrt{\\sigma{}^2_B \\; + \\; \\epsilon{}}}}$\n",
    "\n",
    "        - 이 밖에도 다양한 Normalization 기법들이 있다\n",
    "\n",
    "            <Img src='../image/batch_normaliztion_01.jpeg' height=200>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('m1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdc2b39dba4a1a2bb6d840bc8828727e3a4c4135026ee5b84640756028bbf810"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
