{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 : DL Basic Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 - Introduction\n",
    "- Implementation skills (tools)\n",
    "    - tensorflow\n",
    "    - pytorch\n",
    "- Math skills\n",
    "    - Linear Algrebra (선형대수학)\n",
    "    - Probability (확률론)\n",
    "- Knowing a lot of recent PAPERS (최신논문경향)\n",
    "    - 14 ~ 15 기본 논문내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 - Contents\n",
    "- 방향성\n",
    "    - 각 주제의 간략한 설명\n",
    "    - 해당 주제의 논문들을 찾아가는 방법\n",
    "- 주제들\n",
    "    1. Historycal Review\n",
    "    2. Neural Network & Multi-Layer Perceptron\n",
    "    3. Optimization Methods\n",
    "    4. Convolutional Neural Networks\n",
    "    5. Modern CNN\n",
    "    6. Compoter Vsion Applications\n",
    "    7. Recurrent Neural Network\n",
    "    8. Transfomer\n",
    "    9. Generative Models Part-1\n",
    "    10. Generative Models Part-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 - 인공지능 분류\n",
    "- Altificial Inteligence (AI) (인공지능) : 인간의 지능을 모방하는 것\n",
    "    - Machine Learing (ML) (기계학습) : 데이터 중심 접근법 \n",
    "        - Deep Learing (DL) (딥러닝) : 인공 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 - Deep Learing Key Conmponents\n",
    "- 새로운 논문과 연구를 바라볼떄 이 네가지를 기준으로 바라보자\n",
    "    - The *Data* that the model can learn from : \n",
    "    - The *Model* how to transfrom the data :\n",
    "    - The *Loss* function that quantifies the badness of the model :\n",
    "    - The *Algorithm* to adjust the parameters to minimize the loss :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04_01 : DATA\n",
    "- Data depend on the type of the problem to slove\n",
    "    - Classification\n",
    "    - Semantic Segmentation : 각 이미지의 픽셀별로 분류하는것\n",
    "    - Detection : 이미안의 물체의 바운딩 박스를 찾는것\n",
    "    - Pose Estimation : 이지안의 사람의 2,3차원 스켈레톤 정도를 찾는것\n",
    "    - Visual QnA : 질문에 대한 답을 찾는것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04_02 : Model\n",
    "- 이미지, 단어, 텍스트 등을 원하는 형태의 Label로 바꿔주는 도구\n",
    "    - 모델의 성징에 따라서 결과가 달라지고 그 안에 다양한 테크닉이 있음\n",
    "        - EX\n",
    "            - Alex, GoogLeNet, ResNet, DenseNet, LSTM, Deep AutoEncoders, GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04_03 : Loss function\n",
    "- 이루고자 하는것의 근사치\n",
    "- 모델과 데이터가 절해져있을때 이 모델을 어떻게 학습할지 여부\n",
    "- 해당 loss Function 을 통상적이라는 이유로 무작정 사용하는것이 아닌 사용되는 이유와 필요시 다른 loss Function 을 적용할 수 있어야 한다\n",
    "\n",
    "    - Loss Function EX\n",
    "        - Regression Task\n",
    "            - MSE (Mean Square Error) = $\\displaystyle{\\frac{1}{N}\\sum_{i=1}^{n}\\sum_{d=1}^{D}({y_i}^{(d)}-{\\hat{y}_i}^{(d)})^2}$\n",
    "        - Classification Task\n",
    "            - CE (Cross Entropy) = $\\displaystyle{-\\frac{1}{N}\\sum_{i=1}^{n}\\sum_{d=1}^{D}{y_i}^{(d)}\\log{\\hat{y}_i}^{(d)}}$\n",
    "        - Probabilistic Task\n",
    "            - MLE (Maximum Likelihood Estimation) = $\\displaystyle{\\frac{1}{N}\\sum_{i=1}^{n}\\sum_{d=1}^{D}\\log{\\Nu{}({y_i}^{(d)}\\\\;{\\hat{y}_i}^{(d)},1)} \\quad}$ (=MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04_04 : optimization Algorithm (최적화 방법)\n",
    "- 데이터, 모델, loss func 가 주어져있을때 네트워크를 어떻게 줄일지 고민하는 것\n",
    "- 또한 한번도 보지못한 테스트 데이터에서도 잘 동작할 수 있도록 하는 것\n",
    "\n",
    "    - EX (오버피팅 방지)\n",
    "        - Dropout\n",
    "        - Early stopping\n",
    "        - K-fold validation\n",
    "        - Weight decay\n",
    "        - Batch normalization\n",
    "        - Mixup\n",
    "        - Ensemble\n",
    "        - Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 : Historical DL Trand Review\n",
    "- 2012 - AlexNet\n",
    "- 2013 - DQN\n",
    "- 2014 - Encoder / Decoder / Adam\n",
    "- 2015 - GAN, ResNet(Residual Network)\n",
    "- 2016 - \n",
    "- 2017 - Transformer\n",
    "- 2018 - Bert\n",
    "- 2019 - Big Language Model (GPT-X)\n",
    "- 2020 - Self Supervised Learning (SimCLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 : Neral Network (MLP)\n",
    "- Neural network are computing system veguely inspired by the biological neural networks that constitute animal brains\n",
    "    - 신경망은 동물 뇌를 구성하는 생물학적 신경 네트워크에서 영감을 얻은 컴퓨팅 시스템입니다.\n",
    "- Neural networks are function approximators that stack affine transformations followed by nonlinear transformations\n",
    "    - 신경망은 아핀 변환과 비선형 변환을 쌓는 기능 근사기입니다.\n",
    "    - 뉴럴 네트워크는 함수를 모방하는 function approximator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 - Linear Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's start with the most simple example\n",
    "\n",
    "    - Data : $\\displaystyle{D \\; = \\; \\{(x_i\\,y_i)\\}_{i=1}^N}$\n",
    "    - Model : $\\displaystyle{\\hat{y} \\; = \\; \\vec{w}\\cdot\\vec{x} \\; + \\; \\vec{b}}$\n",
    "    - Loss : $\\displaystyle{\\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^N(y_i \\; - \\; \\hat{y}_i)^2}$\n",
    "\n",
    "        - so, find $\\vec{w} \\; \\& \\; \\vec{b}$\n",
    "\n",
    "        - Partial deivative \n",
    "             > $\\displaystyle{\\frac{\\partial{\\mathcal{L}}}{\\partial{w}} = \\frac{\\partial{}}{\\partial{w}}\\sum_{i=1}^N(y_i \\; - \\; \\hat{y}_i)^2}\\\\$\n",
    "             > $\\displaystyle{\\qquad = \\frac{\\partial{}}{\\partial{w}}\\frac{1}{N}\\sum_{i=1}^N(y_i \\; - \\; wx_i \\; - \\; b)^2}\\\\$\n",
    "             > $\\displaystyle{\\qquad = \\frac{\\partial{}}{\\partial{w}}\\frac{1}{N}\\sum_{i=1}^N(y_i^2 \\; -2x_iy_iw \\; -2by_i \\; + x_i^2w^2 \\; + 2bx_iw \\; + b^2)}\\\\$\n",
    "             > $\\displaystyle{\\qquad = \\frac{\\partial{}}{\\cancel{\\partial{w}}}\\frac{1}{N}\\sum_{i=1}^N(\\cancel{y_i^2} \\; -2x_iy_i\\cancel{w} \\; \\cancel{-2by_i} \\; + 2 \\times x_i^2\\cancel{w^2} \\times w \\; + 2bx_i\\cancel{w} \\; + \\cancel{b^2})}\\\\$\n",
    "             > $\\displaystyle{\\qquad = \\frac{1}{N}\\sum_{i=1}^N(-2y_i \\; + 2x_iw \\; + 2b)x_i}\\\\$\n",
    "             > $\\displaystyle{\\qquad = \\frac{1}{N}\\sum_{i=1}^N-2(y_i \\; - wx_i \\; - b)x_i}$\n",
    "\n",
    "             > $\\displaystyle{\\frac{\\partial{\\mathcal{L}}}{\\partial{b}} = \\frac{\\partial{}}{\\partial{b}}\\sum_{i=1}^N(y_i \\; - \\; \\hat{y}_i)^2}\\\\$\n",
    "             > $\\displaystyle{\\qquad = \\frac{\\partial{}}{\\partial{b}}\\frac{1}{N}\\sum_{i=1}^N(y_i \\; - \\; wx_i \\; - \\; b)^2}\\\\$\n",
    "             > $\\displaystyle{\\qquad = \\frac{1}{N}\\sum_{i=1}^N-2(y_i \\; - wx_i \\; - b)}$\n",
    "\n",
    "             > $\\displaystyle{w \\leftarrow w - \\eta\\frac{\\partial{\\mathcal{L}}}{\\partial{w}}} \\\\$ \n",
    "             > $\\displaystyle{b \\leftarrow b - \\eta\\frac{\\partial{\\mathcal{L}}}{\\partial{b}}} \\\\$\n",
    "             >> - $\\displaystyle{\\leftarrow}$ : Update\n",
    "             >> -  $\\\\\\displaystyle{\\eta}$ : Stepsize (learning rate)\n",
    "\n",
    "- Of course, we cna handel multi dimensional input and out put\n",
    "\n",
    "    > $\\displaystyle{\\mathbb{Y}_{m \\times n} \\; = \\; \\mathbb{X}_{m \\times i} \\cdot \\mathbb{W}^T_{i \\times n} + \\mathbb{B}_{m \\times n}} \\\\$\n",
    "    > $\\displaystyle{\\qquad\\qquad = \\mathbb{X}_{m \\times i}\\overset{\\{\\mathbb{W}_{i \\times n}\\\\,\\mathbb{b}\\}}{\\to}\\mathbb{Y}_{m \\times n}}$\n",
    "\n",
    "    - But, What if we stack more ?\n",
    "        - We need nonlinearity (활성함수를 통한 변환)\n",
    "            - ReLU\n",
    "            - Sigmoid\n",
    "            - Hyperbolic Tangent\n",
    "\n",
    "    - Deep Learning 이 강력한 이유?\n",
    "        - There is a single hidden layer feedforward network that approximates any measureable function to any desired dgree of accuracy on some compact set K.\n",
    "        - 히든 레이어가 하나만 있는 뉴럴 네트워크라 하더라도 모든 선형식을 표현할 수 있다\n",
    "            - 하지만 그 표현식이 우리가 원하는 식이라고는 보장되지 않는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
